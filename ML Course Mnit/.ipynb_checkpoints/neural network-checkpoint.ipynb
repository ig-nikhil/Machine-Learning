{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a55b62-4b18-40a9-bc71-d5ddc3d0f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def initialize_weights(input_dim, hidden_layers, output_dim):\n",
    "    weights = {}\n",
    "    layers_dims = [input_dim] + hidden_layers + [output_dim]\n",
    "    for i in range(1, len(layers_dims)):\n",
    "        weights['W' + str(i)] = np.random.randn(layers_dims[i], layers_dims[i-1]) * 0.01\n",
    "        weights['b' + str(i)] = np.zeros((layers_dims[i], 1))\n",
    "    return weights\n",
    "\n",
    "def forward_propagation(X, weights):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(weights) // 2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        Z = np.dot(weights['W' + str(l)], A_prev) + weights['b' + str(l)]\n",
    "        A = sigmoid(Z)\n",
    "        caches.append((A_prev, Z))\n",
    "    Z_last = np.dot(weights['W' + str(L)], A) + weights['b' + str(L)]\n",
    "    AL = Z_last # Linear activation for the last layer\n",
    "    caches.append((A, Z_last))\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = np.sum((AL - Y) ** 2) / (2 * m)\n",
    "    return cost\n",
    "\n",
    "def backward_propagation(AL, Y, caches, weights):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    dAL = (AL - Y)\n",
    "    current_cache = caches[L - 1]\n",
    "    A_prev, Z_last = current_cache\n",
    "    grads['dW' + str(L)] = np.dot(dAL, A_prev.T) / m\n",
    "    grads['db' + str(L)] = np.sum(dAL, axis=1, keepdims=True) / m\n",
    "    dA = np.dot(weights['W' + str(L)].T, dAL)\n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]\n",
    "        A_prev, Z = current_cache\n",
    "        dZ = dA * sigmoid_derivative(Z)\n",
    "        grads['dW' + str(l + 1)] = np.dot(dZ, A_prev.T) / m\n",
    "        grads['db' + str(l + 1)] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        dA = np.dot(weights['W' + str(l + 1)].T, dZ)\n",
    "    return grads\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
